{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler # Preprocessing recommended by: https://www.geeksforgeeks.org/data-analysis/principal-component-analysis-with-python/\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from NM_classifier import find_nearest_mean\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca58731",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_in = np.genfromtxt(\"train_in.csv\", delimiter=\",\")\n",
    "train_out = np.genfromtxt(\"train_out.csv\", delimiter=\",\")\n",
    "test_in = np.genfromtxt(\"test_in.csv\", delimiter=\",\")\n",
    "test_out = np.genfromtxt(\"test_out.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d228865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1 Problem 1\n",
    "\n",
    "def dist(x, y):\n",
    "    z = x - y\n",
    "    return np.linalg.norm(z)\n",
    "\n",
    "cloud = []\n",
    "\n",
    "for i in range(0, 10):\n",
    "    cloud.append(train_in[np.where(train_out == i)])\n",
    "\n",
    "\n",
    "center = []\n",
    "for j in range(0, 10):\n",
    "    center.append(np.mean(cloud[j], axis=0))\n",
    "\n",
    "np.savetxt(\"cloud_centers.csv\", center, delimiter=\",\")\n",
    "\n",
    "distance = np.zeros((10, 10))\n",
    "for k in range(0, 10):\n",
    "    for l in range(0, 10):\n",
    "        distance[k, l] = dist(center[k], center[l])\n",
    "\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2b8b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1 Problem 2 PCA\n",
    "\n",
    "# Preprocessing recommended by: https://www.geeksforgeeks.org/data-analysis/principal-component-analysis-with-python/\n",
    "# Based on example in: https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py\n",
    "\n",
    "# Preprocessing the scale\n",
    "sc = StandardScaler()\n",
    "train_in = sc.fit_transform(train_in)\n",
    "\n",
    "X_reduced = PCA(n_components=2).fit_transform(train_in)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "PCA_reduced = ax.scatter(X_reduced[:,0], X_reduced[:,1], c=train_out.to_numpy(), alpha=0.6, label=train_out.to_numpy(), cmap=\"Paired\")\n",
    "\n",
    "legend = ax.legend(*PCA_reduced.legend_elements(num=10),\n",
    "                    loc=\"upper right\", title=\"Number\")\n",
    "ax.add_artist(legend)\n",
    "\n",
    "plt.title(\"PCA\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a8ea08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1 Problem 2 U-MAP\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "\n",
    "embedding = reducer.fit_transform(train_in)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(embedding[:, 0],\n",
    "                     embedding[:, 1],\n",
    "                     c = train_out,\n",
    "                     cmap = \"Paired\",\n",
    "                     s = 5)\n",
    "legend = ax.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "ax.add_artist(legend)\n",
    "plt.title(\"U-MAP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1 Problem 2 T-SNE\n",
    "\n",
    "train_in_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "                  init='random', perplexity=10).fit_transform(train_in)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "PCA_reduced = ax.scatter(train_in_embedded[:,0], train_in_embedded[:,1], c=train_out, alpha=0.6, label=train_out, cmap=\"Paired\")\n",
    "\n",
    "legend = ax.legend(*PCA_reduced.legend_elements(num=10),\n",
    "                    loc=\"upper right\", title=\"Number\")\n",
    "ax.add_artist(legend)\n",
    "\n",
    "plt.title(\"T-SNE\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c12a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1 Problem 3\n",
    "\n",
    "#same function as dist for 10 vectors simultaneous\n",
    "def dist_center(x, c):\n",
    "    vector = np.tile(x, (10, 1))\n",
    "    res = vector - c\n",
    "    abs = np.linalg.norm(res, axis=1)\n",
    "    return np.where(abs == np.min(abs))[0][0] #output only integer where distance is smallest\n",
    "\n",
    "def find_nearest_mean(X, c):\n",
    "    nearest_mean = []\n",
    "    for x in X:\n",
    "        nearest_mean.append(dist_center(x, c).item())\n",
    "    return nearest_mean\n",
    "\n",
    "centers = np.genfromtxt(\"cloud_centers.csv\", delimiter=\",\")\n",
    "\n",
    "train_nearest_mean = []\n",
    "for i in range(len(train_in)):\n",
    "    train_nearest_mean.append(dist_center(train_in[i], centers).item())\n",
    "\n",
    "train_correct = np.where(train_nearest_mean == train_out)[0]\n",
    "train_percentage = 100 * len(train_correct) / len(train_in)\n",
    "print(f\"{train_percentage:.2f}% from the train set is correctly classified using the nearest mean method.\")\n",
    "\n",
    "test_nearest_mean = []\n",
    "for i in range(len(test_in)):\n",
    "    test_nearest_mean.append(dist_center(test_in[i], centers).item())\n",
    "\n",
    "test_correct = np.where(test_nearest_mean == test_out)[0]\n",
    "test_percentage = 100 * len(test_correct) / len(test_in)\n",
    "print(f\"{test_percentage:.2f}% from the test set is correctly classified using the nearest mean method.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1 Problem 4\n",
    "# Based on example given in: https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "\n",
    "centers = np.genfromtxt(\"cloud_centers.csv\", delimiter=\",\")\n",
    "\n",
    "# We only fit the KNN classifier with the training data.\n",
    "neigh = KNeighborsClassifier()\n",
    "neigh.fit(train_in, train_out)\n",
    "\n",
    "KNN_train_classification = []\n",
    "for number in train_in:\n",
    "    KNN_train_classification.append(neigh.predict([number])[0])\n",
    "\n",
    "KNN_test_classification = []\n",
    "for number in test_in:\n",
    "    KNN_test_classification.append(neigh.predict([number])[0])\n",
    "\n",
    "nearest_mean_train_classification = find_nearest_mean(train_in, centers)\n",
    "nearest_mean_test_classification = find_nearest_mean(test_in, centers)\n",
    "\n",
    "def plot_cf_matrix(ground_truth, prediction, title):\n",
    "    cf_matrix = confusion_matrix(ground_truth, prediction)  # Ground truth values go first!\n",
    "    cf_matrix = cf_matrix.astype(float)\n",
    "    \n",
    "    for i in range(cf_matrix.shape[0]):\n",
    "        total = sum(cf_matrix[i, :])\n",
    "        cf_matrix[i, :] = cf_matrix[i, :] / total\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cf_matrix, annot=True, fmt='.1%', linewidths=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "\n",
    "plot_cf_matrix(train_out, nearest_mean_train_classification, \"Confustion Matrix for NM Classification on Training Data\")\n",
    "plot_cf_matrix(train_out, KNN_train_classification, \"Confustion Matrix for KNN Classification on Training Data\")\n",
    "plot_cf_matrix(test_out, nearest_mean_test_classification, \"Confustion Matrix for NM Classification on Test Data\")\n",
    "plot_cf_matrix(test_out, KNN_test_classification, \"Confustion Matrix for KNN Classification on Test Data\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5bc02f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.818006196867037\n",
      "Loss: 18.343814690603185\n",
      "Loss: 28.24369513126831\n",
      "Loss: 21.124916755422223\n",
      "Loss: 12.892418310499421\n",
      "Loss: 12.031787085160081\n",
      "Loss: 7.9748445284568215\n",
      "Loss: 5.0903696673956365\n",
      "Loss: 2.888498597302475\n",
      "Loss: 2.5777751951423027\n",
      "Loss: 4.2419181620940085\n",
      "Loss: 1.8498870608256905\n",
      "Loss: 0.9747330341449181\n",
      "Loss: 0.7857181875004329\n",
      "Loss: 0.7714538999080771\n",
      "Loss: 0.7164516408319309\n",
      "Loss: 0.6514250729552544\n",
      "Loss: 0.5602391282777899\n",
      "Loss: 0.4861573724044825\n",
      "Loss: 0.4107354838648178\n",
      "Loss: 0.3629513182265863\n",
      "Loss: 0.33993163140811505\n",
      "Loss: 0.32348386782553135\n",
      "Loss: 0.3095316179410478\n",
      "Loss: 0.2968766272332442\n",
      "Loss: 0.2853059777085597\n",
      "Loss: 0.2745508853889111\n",
      "Loss: 0.26448688221651545\n",
      "Loss: 0.25501467465278177\n",
      "Loss: 0.24606914845823705\n",
      "Loss: 0.2376007930611053\n",
      "Loss: 0.22956969653133455\n",
      "Loss: 0.22194108552134328\n",
      "Loss: 0.21468345045373516\n",
      "Loss: 0.2077680608069875\n",
      "Loss: 0.20116912375703347\n",
      "Loss: 0.19486391428351088\n",
      "Loss: 0.18883271320667197\n",
      "Loss: 0.1830585061956477\n",
      "Loss: 0.1775265601639665\n",
      "Loss: 0.17222398990438315\n",
      "Loss: 0.16713939673808417\n",
      "Loss: 0.16226261696513264\n",
      "Loss: 0.1575845558327404\n",
      "Loss: 0.15309705260527404\n",
      "Loss: 0.1487927137881162\n",
      "Loss: 0.14466468331653748\n",
      "Loss: 0.1407063650784509\n",
      "Loss: 0.13691115563584064\n",
      "Loss: 0.1332722559328614\n",
      "Loss: 0.12978260582358717\n",
      "Loss: 0.12643494038628678\n",
      "Loss: 0.12322193030932035\n",
      "Loss: 0.12013635575582766\n",
      "Loss: 0.1171712708817254\n",
      "Loss: 0.1143201321109252\n",
      "Loss: 0.11157687834637442\n",
      "Loss: 0.10893596260981314\n",
      "Loss: 0.10639234219423785\n",
      "Loss: 0.10394143858233457\n",
      "Loss: 0.10157907946433173\n",
      "Loss: 0.09930143391238724\n",
      "Loss: 0.09710494919538575\n",
      "Loss: 0.09498629483166451\n",
      "Loss: 0.09294231690347378\n",
      "Loss: 0.09097000364040288\n",
      "Loss: 0.08906646182659414\n",
      "Loss: 0.08722890262528259\n",
      "Loss: 0.08545463487350004\n",
      "Loss: 0.08374106371826248\n",
      "Loss: 0.08208569257049592\n",
      "Loss: 0.08048612665356333\n",
      "Loss: 0.0789400768242136\n",
      "Loss: 0.07744536276373144\n",
      "Loss: 0.07599991501855152\n",
      "Loss: 0.07460177567847502\n",
      "Loss: 0.07324909770080185\n",
      "Loss: 0.07194014301708922\n",
      "Loss: 0.07067327960310743\n",
      "Loss: 0.06944697766799425\n",
      "Loss: 0.0682598050491962\n",
      "Loss: 0.06711042181393385\n",
      "Loss: 0.06599757399495695\n",
      "Loss: 0.06492008635367298\n",
      "Loss: 0.06387685408416\n",
      "Loss: 0.0628668334518879\n",
      "Loss: 0.06188903149238566\n",
      "Loss: 0.060942495056276844\n",
      "Loss: 0.06002629964767215\n",
      "Loss: 0.059139538629228626\n",
      "Loss: 0.05828131342958961\n",
      "Loss: 0.057450725368727584\n",
      "Loss: 0.05664686961105344\n",
      "Loss: 0.05586883157884567\n",
      "Loss: 0.055115685936600725\n",
      "Loss: 0.054386498024081485\n",
      "Loss: 0.05368032740552372\n",
      "Loss: 0.05299623304183778\n",
      "Loss: 0.05233327949910385\n",
      "Loss: 0.05169054358617412\n",
      "Loss: 0.05106712086141865\n",
      "Loss: 0.05046213154869216\n",
      "Loss: 0.049874725534402975\n",
      "Loss: 0.04930408625844076\n",
      "Loss: 0.04874943344178666\n",
      "Loss: 0.04821002469902838\n",
      "Loss: 0.047685156158062555\n",
      "Loss: 0.04717416225183659\n",
      "Loss: 0.046676414862480815\n",
      "Loss: 0.046191321993365876\n",
      "Loss: 0.045718326126648674\n",
      "Loss: 0.04525690239901883\n",
      "Loss: 0.044806556701428406\n",
      "Loss: 0.0443668237827834\n",
      "Loss: 0.04393726541474499\n",
      "Loss: 0.04351746865572108\n",
      "Loss: 0.04310704423693191\n",
      "Loss: 0.042705625081792126\n",
      "Loss: 0.04231286496126038\n",
      "Loss: 0.04192843728169309\n",
      "Loss: 0.04155203399755833\n",
      "Loss: 0.04118336463863268\n",
      "Loss: 0.04082215543961843\n",
      "Loss: 0.04046814855915803\n",
      "Loss: 0.04012110137474005\n",
      "Loss: 0.039780785839805495\n",
      "Loss: 0.039446987889349\n",
      "Loss: 0.03911950688039509\n",
      "Loss: 0.03879815505387839\n",
      "Loss: 0.038482757004680124\n",
      "Loss: 0.03817314914690983\n",
      "Loss: 0.03786917916203964\n",
      "Loss: 0.03757070541829497\n",
      "Loss: 0.03727759635088173\n",
      "Loss: 0.03698972979429867\n",
      "Loss: 0.03670699226023599\n",
      "Loss: 0.03642927815746002\n",
      "Loss: 0.03615648895363726\n",
      "Loss: 0.03588853228318935\n",
      "Loss: 0.035625321009838846\n",
      "Loss: 0.035366772257245264\n",
      "Loss: 0.03511280642569575\n",
      "Loss: 0.03486334621678906\n",
      "Loss: 0.03461831569099356\n",
      "Loss: 0.03437763938446009\n",
      "Loss: 0.03414124151121271\n",
      "Loss: 0.03390904527466428\n",
      "Loss: 0.033680972308355234\n",
      "Loss: 0.033456942260144305\n",
      "Loss: 0.033236872527255136\n",
      "Loss: 0.03302067814219546\n",
      "Loss: 0.03280827180229175\n",
      "Loss: 0.03259956402905662\n",
      "Loss: 0.032394463438366224\n",
      "Loss: 0.032192877098820225\n",
      "Loss: 0.031994710953846435\n",
      "Loss: 0.03179987028303575\n",
      "Loss: 0.03160826017963795\n",
      "Loss: 0.03141978602377076\n",
      "Loss: 0.03123435393430463\n",
      "Loss: 0.031051871186185662\n",
      "Loss: 0.030872246583801704\n",
      "Loss: 0.030695390784599798\n",
      "Loss: 0.03052121657033689\n",
      "Loss: 0.030349639065973443\n",
      "Loss: 0.0301805759082649\n",
      "Loss: 0.030013947367583827\n",
      "Loss: 0.029849676427468715\n",
      "Loss: 0.029687688826923228\n",
      "Loss: 0.029527913070665927\n",
      "Loss: 0.02937028041244093\n",
      "Loss: 0.029214724816221043\n",
      "Loss: 0.029061182899735184\n",
      "Loss: 0.02890959386428125\n",
      "Loss: 0.028759899414290357\n",
      "Loss: 0.028612043669615218\n",
      "Loss: 0.028465973073046026\n",
      "Loss: 0.028321636295126525\n",
      "Loss: 0.028178984137955596\n",
      "Loss: 0.028037969439319098\n",
      "Loss: 0.027898546978204656\n",
      "Loss: 0.027760673382501842\n",
      "Loss: 0.027624307039482786\n",
      "Loss: 0.027489408009485898\n",
      "Loss: 0.02735593794308523\n",
      "Loss: 0.027223860001915862\n",
      "Loss: 0.027093138783235642\n",
      "Loss: 0.02696374024823503\n",
      "Loss: 0.02683563165405221\n",
      "Loss: 0.026708781489411967\n",
      "Loss: 0.026583159413776154\n",
      "Loss: 0.026458736199874617\n",
      "Loss: 0.02633548367947077\n",
      "Loss: 0.026213374692208448\n",
      "Loss: 0.026092383037383367\n",
      "Loss: 0.025972483428481127\n",
      "Loss: 0.025853651450326143\n",
      "Loss: 0.025735863518689645\n",
      "Loss: 0.025619096842210278\n",
      "Loss: 0.02550332938648549\n",
      "Loss: 0.025388539840200505\n",
      "Loss: 0.025274707583166172\n",
      "Loss: 0.025161812656146285\n",
      "Loss: 0.025049835732359776\n",
      "Loss: 0.024938758090551877\n",
      "Loss: 0.024828561589533345\n",
      "Loss: 0.024719228644095058\n",
      "Loss: 0.024610742202209386\n",
      "Loss: 0.024503085723437283\n",
      "Loss: 0.024396243158464753\n",
      "Loss: 0.02429019892969721\n",
      "Loss: 0.024184937912846028\n",
      "Loss: 0.02408044541944528\n",
      "Loss: 0.023976707180241297\n",
      "Loss: 0.023873709329401693\n",
      "Loss: 0.023771438389493936\n",
      "Loss: 0.02366988125718712\n",
      "Loss: 0.02356902518963382\n",
      "Loss: 0.023468857791491965\n",
      "Loss: 0.02336936700254885\n",
      "Loss: 0.023270541085912692\n",
      "Loss: 0.023172368616739252\n",
      "Loss: 0.0230748384714627\n",
      "Loss: 0.0229779398175028\n",
      "Loss: 0.022881662103421768\n",
      "Loss: 0.022785995049506003\n",
      "Loss: 0.02269092863874981\n",
      "Loss: 0.022596453108219165\n",
      "Loss: 0.022502558940775536\n",
      "Loss: 0.02240923685714083\n",
      "Loss: 0.022316477808285645\n",
      "Loss: 0.02222427296812402\n",
      "Loss: 0.022132613726499634\n",
      "Loss: 0.022041491682448164\n",
      "Loss: 0.021950898637722642\n",
      "Loss: 0.021860826590568407\n",
      "Loss: 0.021771267729736078\n",
      "Loss: 0.021682214428720637\n",
      "Loss: 0.021593659240216178\n",
      "Loss: 0.021505594890776313\n",
      "Loss: 0.021418014275670395\n",
      "Loss: 0.02133091045392694\n",
      "Loss: 0.021244276643555722\n",
      "Loss: 0.021158106216940444\n",
      "Loss: 0.02107239269639461\n",
      "Loss: 0.020987129749873636\n",
      "Loss: 0.020902311186836137\n",
      "Loss: 0.020817930954248557\n",
      "Loss: 0.02073398313272681\n",
      "Loss: 0.020650461932809436\n",
      "Loss: 0.02056736169135704\n",
      "Loss: 0.020484676868072742\n",
      "Loss: 0.020402402042139105\n",
      "Loss: 0.02032053190896696\n",
      "Loss: 0.020239061277051763\n",
      "Loss: 0.020157985064933624\n",
      "Loss: 0.02007729829825703\n",
      "Loss: 0.01999699610692674\n",
      "Loss: 0.019917073722356415\n",
      "Loss: 0.01983752647480661\n",
      "Loss: 0.01975834979080934\n",
      "Loss: 0.019679539190675915\n",
      "Loss: 0.019601090286085532\n",
      "Loss: 0.0195229987777521\n",
      "Loss: 0.019445260453166368\n",
      "Loss: 0.019367871184411526\n",
      "Loss: 0.019290826926049633\n",
      "Loss: 0.019214123713076976\n",
      "Loss: 0.019137757658946156\n",
      "Loss: 0.019061724953653218\n",
      "Loss: 0.01898602186188776\n",
      "Loss: 0.018910644721244388\n",
      "Loss: 0.018835589940493835\n",
      "Loss: 0.018760853997912372\n",
      "Loss: 0.01868643343966762\n",
      "Loss: 0.018612324878259694\n",
      "Loss: 0.018538524991016183\n",
      "Loss: 0.018465030518639756\n",
      "Loss: 0.018391838263807025\n",
      "Loss: 0.01831894508981766\n",
      "Loss: 0.01824634791929258\n",
      "Loss: 0.018174043732920156\n",
      "Loss: 0.018102029568249595\n",
      "Loss: 0.018030302518530137\n",
      "Loss: 0.01795885973159578\n",
      "Loss: 0.017887698408794162\n",
      "Loss: 0.017816815803959076\n",
      "Loss: 0.017746209222425802\n",
      "Loss: 0.01767587602008856\n",
      "Loss: 0.01760581360249938\n",
      "Loss: 0.017536019424007915\n",
      "Loss: 0.01746649098694146\n",
      "Loss: 0.017397225840824777\n",
      "Loss: 0.01732822158163924\n",
      "Loss: 0.017259475851120906\n",
      "Loss: 0.017190986336097055\n",
      "Loss: 0.017122750767860996\n",
      "Loss: 0.01705476692158477\n",
      "Loss: 0.016987032615769667\n",
      "Loss: 0.01691954571173433\n",
      "Correct results for the training set: \n",
      "0.9994141769185706\n",
      "Correct results for the test set: \n",
      "0.879\n"
     ]
    }
   ],
   "source": [
    "#Part 2\n",
    "\n",
    "# Here is our implementation of the multiclass perceptron:\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "train_in = np.genfromtxt(\"train_in.csv\", delimiter=\",\")\n",
    "train_out = np.genfromtxt(\"train_out.csv\", delimiter=\",\")\n",
    "test_in = np.genfromtxt(\"test_in.csv\", delimiter=\",\")\n",
    "test_out = np.genfromtxt(\"test_out.csv\", delimiter=\",\")\n",
    "\n",
    "bias = np.ones((len(train_in), 1))\n",
    "T = np.append(train_in, bias, axis=1)\n",
    "\n",
    "# Construct onehot matrix with a 1 in each row corresponding to the correct output of the training data.\n",
    "onehots = np.zeros((len(train_in), 10))\n",
    "\n",
    "for i in range(len(train_in)):\n",
    "    j = int(train_out[i])\n",
    "    onehots[i][j] = 1\n",
    "\n",
    "def compute_softmaxes(W):\n",
    "    softmaxes = np.zeros((len(train_in), 10))\n",
    "\n",
    "    for i in range(len(train_in)):\n",
    "        for k in range(10):\n",
    "            softmaxes[i][k] = math.e ** np.dot(W[k], np.transpose(T[i]))\n",
    "\n",
    "        softmaxes[i] /= sum(softmaxes[i])\n",
    "    \n",
    "    return softmaxes\n",
    "\n",
    "def compute_gradients(i, softmaxes):\n",
    "    gradients = np.zeros((10, 257))\n",
    "    for k in range(10):\n",
    "        gradients[k] = 2 * T[i] * (softmaxes[i][k] - onehots[i][k])\n",
    "\n",
    "    return gradients\n",
    "\n",
    "step_size = 1\n",
    "\n",
    "# Initialise the weights matrix with random values between -0.5 and 0.5\n",
    "W = np.random.rand(10, 257)\n",
    "W = np.subtract(W, np.ones((10, 257)) / 2)\n",
    "\n",
    "# Training loop with epochs\n",
    "for epoch in range(300):\n",
    "    softmaxes = compute_softmaxes(W)\n",
    "    total_gradients = np.zeros((10, 257))\n",
    "    for i in range(len(train_in)):\n",
    "        total_gradients += compute_gradients(i, softmaxes)\n",
    "\n",
    "    total_gradients = total_gradients / len(train_in)\n",
    "\n",
    "    W -= total_gradients * step_size\n",
    "    print(\"Loss: \" + str(-sum(sum(onehots * np.log(softmaxes))) / len(train_in)))\n",
    "\n",
    "results = np.zeros(len(train_in))\n",
    "\n",
    "# Validate the results for the training data\n",
    "\n",
    "for i in range(len(train_in)):\n",
    "    output = np.argmax(np.dot(W, np.transpose(T[i])))\n",
    "    if output == train_out[i]:\n",
    "        results[i] = 1\n",
    "    else:\n",
    "        results[i] = 0\n",
    "\n",
    "print(\"Correct results for the training set: \")\n",
    "print(float(sum(results)) / len(train_in))\n",
    "\n",
    "\n",
    "# Validate the results for the test data\n",
    "results = np.zeros(len(test_in))\n",
    "\n",
    "bias = np.ones((len(test_in), 1))\n",
    "T = np.append(test_in, bias, axis=1)\n",
    "\n",
    "for i in range(len(test_in)):\n",
    "    output = np.argmax(np.dot(W, np.transpose(T[i])))\n",
    "    if output == test_out[i]:\n",
    "        results[i] = 1\n",
    "    else:\n",
    "        results[i] = 0\n",
    "\n",
    "print(\"Correct results for the test set: \")\n",
    "print(float(sum(results)) / len(test_in))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
